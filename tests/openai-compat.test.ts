/**
 * Tests for OpenAI-compatible /v1/chat/completions endpoint.
 * These verify the format translation layer works correctly.
 */
import { describe, test, expect } from "bun:test"
import { createProxyServer } from "../src/proxy/server"

const { app } = createProxyServer({ port: 0, host: "127.0.0.1" })

async function req(path: string, init?: RequestInit) {
  return app.fetch(new Request(`http://localhost${path}`, init))
}

async function json(path: string, init?: RequestInit): Promise<{ status: number; body: any }> {
  const res = await req(path, init)
  return { status: res.status, body: await res.json() }
}

// ── OpenAI model listing ────────────────────────────────────────────────────

describe("GET /v1/chat/models", () => {
  test("returns OpenAI-format model list", async () => {
    const { status, body } = await json("/v1/chat/models")
    expect(status).toBe(200)
    expect(body.object).toBe("list")
    expect(body.data).toBeArray()
    expect(body.data.length).toBeGreaterThan(0)
    for (const model of body.data) {
      expect(model.object).toBe("model")
      expect(model.id).toBeDefined()
      expect(model.owned_by).toBe("anthropic")
      expect(typeof model.created).toBe("number")
    }
  })
})

// ── Request validation ──────────────────────────────────────────────────────

describe("POST /v1/chat/completions (validation)", () => {
  test("handles empty messages gracefully", async () => {
    const { status } = await json("/v1/chat/completions", {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({
        model: "claude-sonnet-4-6",
        messages: []
      })
    })
    // Should forward to Anthropic handler which returns 400
    expect(status).toBe(400)
  })
})

// ── Tool format conversion ──────────────────────────────────────────────────

describe("POST /v1/chat/completions (with tools)", () => {
  test("accepts OpenAI-format tools and translates to Anthropic format", async () => {
    // This will return 400 because messages is empty, but validates the tools
    // are accepted without error in the request parsing
    const { status } = await json("/v1/chat/completions", {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({
        model: "claude-sonnet-4-6",
        messages: [{ role: "user", content: "What's the weather?" }],
        tools: [{
          type: "function",
          function: {
            name: "get_weather",
            description: "Get current weather",
            parameters: {
              type: "object",
              properties: {
                location: { type: "string", description: "City name" }
              },
              required: ["location"]
            }
          }
        }],
        stream: false
      })
    })
    // Status would be 200 if proxy is running, or error from SDK
    // The key thing is it doesn't return 400 (our validation passed)
    expect(status).not.toBe(400)
  }, 120_000)

  test("handles tool role messages in conversation", async () => {
    // Validates tool result messages don't cause parsing errors
    const { status } = await json("/v1/chat/completions", {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({
        model: "claude-sonnet-4-6",
        messages: [
          { role: "user", content: "What's the weather?" },
          {
            role: "assistant",
            content: null,
            tool_calls: [{
              id: "call_123",
              type: "function",
              function: { name: "get_weather", arguments: "{\"location\": \"NYC\"}" }
            }]
          },
          {
            role: "tool",
            tool_call_id: "call_123",
            content: "72°F and sunny"
          }
        ],
        stream: false
      })
    })
    // Should not return 400 (validation error)
    expect(status).not.toBe(400)
  }, 120_000)
})

// ── Integration tests (need running proxy) ─────────────────────────────────

const PROXY_URL = process.env.PROXY_URL || "http://127.0.0.1:3456"
const SKIP = !process.env.INTEGRATION

function skipUnlessIntegration() {
  if (SKIP) {
    console.log("  [SKIPPED - set INTEGRATION=1 to run]")
    return true
  }
  return false
}

describe("Integration: OpenAI non-streaming", () => {
  test("simple chat completion", async () => {
    if (skipUnlessIntegration()) return
    const res = await fetch(`${PROXY_URL}/v1/chat/completions`, {
      method: "POST",
      headers: { "Content-Type": "application/json", Authorization: "Bearer dummy" },
      body: JSON.stringify({
        model: "claude-sonnet-4-6",
        messages: [{ role: "user", content: "What is 3+3? Reply with ONLY the number." }],
        stream: false
      })
    })
    const body = await res.json() as any
    expect(res.status).toBe(200)
    expect(body.object).toBe("chat.completion")
    expect(body.choices).toBeArray()
    expect(body.choices[0].message.role).toBe("assistant")
    expect(body.choices[0].message.content).toContain("6")
    expect(body.choices[0].finish_reason).toBe("stop")
    expect(body.usage).toBeDefined()
    expect(body.usage.total_tokens).toBeGreaterThan(0)
  }, 120_000)

  test("system message is forwarded", async () => {
    if (skipUnlessIntegration()) return
    const res = await fetch(`${PROXY_URL}/v1/chat/completions`, {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({
        model: "claude-sonnet-4-6",
        messages: [
          { role: "system", content: "Reply with 'PONG' and nothing else." },
          { role: "user", content: "PING" }
        ],
        stream: false
      })
    })
    const body = await res.json() as any
    expect(body.choices[0].message.content.toUpperCase()).toContain("PONG")
  }, 120_000)
})

describe("Integration: OpenAI streaming", () => {
  test("streams chat completion chunks with role delta", async () => {
    if (skipUnlessIntegration()) return
    const res = await fetch(`${PROXY_URL}/v1/chat/completions`, {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({
        model: "claude-sonnet-4-6",
        messages: [{ role: "user", content: "What is 2+2? Reply with ONLY the number." }],
        stream: true
      })
    })
    const text = await res.text()
    const lines = text.split("\n").filter(l => l.startsWith("data: "))
    expect(lines.length).toBeGreaterThan(0)

    // Parse all chunks
    const chunks = lines
      .filter(l => l !== "data: [DONE]")
      .map(l => { try { return JSON.parse(l.slice(6)) } catch { return null } })
      .filter(Boolean)

    // First chunk should have role: "assistant" in delta
    const roleChunk = chunks.find((c: any) => c.choices?.[0]?.delta?.role === "assistant")
    expect(roleChunk).toBeDefined()
    expect(roleChunk.object).toBe("chat.completion.chunk")

    // Should have content chunks
    const contentChunks = chunks.filter((c: any) => c.choices?.[0]?.delta?.content)
    expect(contentChunks.length).toBeGreaterThan(0)

    // Check for [DONE] terminator
    expect(lines.some(l => l === "data: [DONE]")).toBe(true)

    // Final chunk should have finish_reason: "stop"
    const finishChunk = chunks.find((c: any) => c.choices?.[0]?.finish_reason === "stop")
    expect(finishChunk).toBeDefined()

    // Consistent chat ID across chunks
    const ids = new Set(chunks.map((c: any) => c.id))
    expect(ids.size).toBe(1)
  }, 120_000)

  test("stream_options.include_usage returns usage in final chunk", async () => {
    if (skipUnlessIntegration()) return
    const res = await fetch(`${PROXY_URL}/v1/chat/completions`, {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({
        model: "claude-sonnet-4-6",
        messages: [{ role: "user", content: "Say hello." }],
        stream: true,
        stream_options: { include_usage: true }
      })
    })
    const text = await res.text()
    const lines = text.split("\n").filter(l => l.startsWith("data: ") && l !== "data: [DONE]")
    const chunks = lines.map(l => { try { return JSON.parse(l.slice(6)) } catch { return null } }).filter(Boolean)

    // Final chunk with finish_reason should include usage
    const finishChunk = chunks.find((c: any) => c.choices?.[0]?.finish_reason)
    expect(finishChunk).toBeDefined()
    expect(finishChunk.usage).toBeDefined()
    expect(finishChunk.usage.prompt_tokens).toBeGreaterThanOrEqual(0)
    expect(finishChunk.usage.completion_tokens).toBeGreaterThanOrEqual(0)
    expect(finishChunk.usage.total_tokens).toBe(finishChunk.usage.prompt_tokens + finishChunk.usage.completion_tokens)
  }, 120_000)
})
