import { Hono } from "hono"
import { cors } from "hono/cors"
import { query } from "@anthropic-ai/claude-agent-sdk"
import type { Context } from "hono"
import type { ProxyConfig } from "./types"
import { DEFAULT_PROXY_CONFIG } from "./types"
import { claudeLog } from "../logger"
import { execSync } from "child_process"
import { existsSync, writeFileSync, unlinkSync, readFileSync } from "fs"
import { tmpdir } from "os"
import { randomBytes } from "crypto"
import { fileURLToPath } from "url"
import { join, dirname } from "path"
import { createMcpServer } from "../mcpTools"

const PROXY_VERSION: string = (() => {
  try {
    const pkg = JSON.parse(readFileSync(join(dirname(fileURLToPath(import.meta.url)), "../../package.json"), "utf-8"))
    return pkg.version ?? "unknown"
  } catch { return "unknown" }
})()

// Built-in Claude Code tools to block. "Read" is intentionally kept so Claude
// can open image temp-files saved from inbound media blocks.
const BLOCKED_BUILTIN_TOOLS = [
  "Write", "Edit", "MultiEdit",
  "Bash", "Glob", "Grep", "NotebookEdit",
  "WebFetch", "WebSearch", "TodoWrite"
]

const MCP_SERVER_NAME = "opencode"

const ALLOWED_MCP_TOOLS = [
  `mcp__${MCP_SERVER_NAME}__read`,
  `mcp__${MCP_SERVER_NAME}__write`,
  `mcp__${MCP_SERVER_NAME}__edit`,
  `mcp__${MCP_SERVER_NAME}__bash`,
  `mcp__${MCP_SERVER_NAME}__glob`,
  `mcp__${MCP_SERVER_NAME}__grep`,
  `mcp__${MCP_SERVER_NAME}__message`,
]

// Injected after the system prompt in agent mode. Bridges openclaw's "message"
// tool to the actual MCP name and provides clear agentic workflow instructions.
const SEND_MESSAGE_NOTE = `
## Agent context — tool instructions
You are running as an agent with MCP tools. ALWAYS follow this workflow:

1. **Do the work with tools**: Use mcp__opencode__bash, mcp__opencode__write, mcp__opencode__read, etc. to actually complete the task. Do NOT just acknowledge and stop.
2. **Send replies via mcp__opencode__message** (the only way messages reach the user):
   - Text: \`mcp__opencode__message\` with \`to\` (chat ID from conversation_label, e.g. "-1001426819337"), \`message\` = text
   - Files/images: write the file to /tmp/ first, then \`mcp__opencode__message\` with \`filePath\` = absolute path (e.g. "/tmp/robot.png")
3. **After all mcp__opencode__message calls finish, output ONLY: NO_REPLY**

CRITICAL: Your text output is NOT delivered to the user — only mcp__opencode__message calls reach Telegram. Do NOT send a text acknowledgment and stop. Complete the full task with tools first, then send the result, then output NO_REPLY.

System capabilities: python3, PIL/Pillow, rsvg-convert (SVG→PNG), ImageMagick (convert). cairosvg is NOT installed. For SVG to PNG: use \`rsvg-convert input.svg -o output.png\` via bash.`

function resolveClaudeExecutable(): string {
  try {
    const sdkPath = fileURLToPath(import.meta.resolve("@anthropic-ai/claude-agent-sdk"))
    const sdkCliJs = join(dirname(sdkPath), "cli.js")
    if (existsSync(sdkCliJs)) return sdkCliJs
  } catch {}
  try {
    const claudePath = execSync("which claude", { encoding: "utf-8" }).trim()
    if (claudePath && existsSync(claudePath)) return claudePath
  } catch {}
  throw new Error("Could not find Claude Code executable. Install: npm install -g @anthropic-ai/claude-code")
}

const claudeExecutable = resolveClaudeExecutable()

function mapModelToClaudeModel(model: string): "sonnet" | "opus" | "haiku" {
  if (model.includes("opus")) return "opus"
  if (model.includes("haiku")) return "haiku"
  return "sonnet"
}

// ── Content-block serialization ──────────────────────────────────────────────

function saveImageToTemp(block: any, tempFiles: string[]): string | null {
  try {
    let data: string | undefined
    let mediaType = "image/jpeg"

    if (typeof block.data === "string") {
      // openclaw format: { type: "image", data: "<base64>", media_type?: "…" }
      data = block.data
      mediaType = block.media_type || mediaType
    } else if (block.source) {
      // Anthropic API format: { type: "image", source: { type: "base64", data, media_type } }
      if (block.source.type === "base64" && block.source.data) {
        data = block.source.data
        mediaType = block.source.media_type || mediaType
      } else if (block.source.url) {
        return block.source.url
      }
    }

    if (!data) return null

    const ext = mediaType.split("/")[1]?.replace("jpeg", "jpg") || "jpg"
    const tmpPath = join(tmpdir(), `openclaw-img-${randomBytes(8).toString("hex")}.${ext}`)
    writeFileSync(tmpPath, Buffer.from(data, "base64"))
    tempFiles.push(tmpPath)
    return tmpPath
  } catch {
    return null
  }
}

function serializeBlock(block: any, tempFiles: string[]): string {
  switch (block.type) {
    case "text":
      return block.text || ""
    case "image": {
      const imgPath = saveImageToTemp(block, tempFiles)
      return imgPath ? `[Image: ${imgPath}]` : "[Image: (unable to save)]"
    }
    case "tool_use":
      return `[Tool call: ${block.name}\nInput: ${JSON.stringify(block.input ?? {}, null, 2)}]`
    case "tool_result": {
      // Truncate large tool results — cap at 4000 chars to prevent context explosion.
      const content = Array.isArray(block.content)
        ? block.content.filter((b: any) => b.type === "text").map((b: any) => b.text).join("")
        : String(block.content ?? "")
      const truncated = content.length > 4000
        ? content.slice(0, 4000) + `\n...[truncated ${content.length - 4000} chars]`
        : content
      return `[Tool result (id=${block.tool_use_id}): ${truncated}]`
    }
    case "thinking":
      return ""
    default:
      return ""
  }
}

function serializeContent(content: string | Array<any>, tempFiles: string[]): string {
  if (typeof content === "string") return content
  if (!Array.isArray(content)) return String(content)
  return content.map(b => serializeBlock(b, tempFiles)).filter(Boolean).join("\n")
}

function cleanupTempFiles(tempFiles: string[]) {
  for (const f of tempFiles) {
    try { unlinkSync(f) } catch {}
  }
}

// ── Response normalization ───────────────────────────────────────────────────

// If the agent used the message tool it may output verbose text before NO_REPLY.
// openclaw's isSilentReplyText handles "ends with NO_REPLY", but we normalize to
// just "NO_REPLY" for a clean session history.
// If the agent produced no text (maxTurns exhausted or empty run), return "..."
// so openclaw shows something rather than silence.
function normalizeResponse(text: string): string {
  if (!text || !text.trim()) return "..."
  const trimmed = text.trimEnd()
  if (trimmed.endsWith("NO_REPLY")) return "NO_REPLY"
  return text
}

// ── Client tool-use support ──────────────────────────────────────────────────
// When the caller provides tool definitions (e.g. Claude Code, LangChain, etc.)
// we switch to single-turn mode: inject tool defs into the system prompt, run
// one LLM turn, parse <tool_use> blocks from the output, and return them as
// proper Anthropic tool_use content blocks.  This lets the caller manage the
// tool loop themselves, exactly as the real Anthropic API behaves.
//
// We stay in agent mode (multi-turn MCP) when:
//   • No tools in the request, OR
//   • The system prompt looks like an openclaw session (which has its own tools)

function isClientToolMode(body: any): boolean {
  if (!body.tools?.length) return false
  // Conversation already contains tool_result → client is managing the loop
  if (body.messages?.some((m: any) =>
    Array.isArray(m.content) && m.content.some((b: any) => b.type === "tool_result")
  )) return true
  // System prompt has openclaw markers → stay in agent mode
  const sysText = Array.isArray(body.system)
    ? body.system.filter((b: any) => b.type === "text").map((b: any) => b.text).join(" ")
    : String(body.system ?? "")
  if (sysText.includes("conversation_label") || sysText.includes("chat id:")) return false
  return true
}

function buildClientToolsPrompt(tools: any[]): string {
  const defs = tools.map((t: any) => {
    const schema = t.input_schema ? `\nInput schema:\n${JSON.stringify(t.input_schema, null, 2)}` : ""
    return `### ${t.name}\n${t.description ?? ""}${schema}`
  }).join("\n\n")
  return `\n\n## Available Tools\n\nTo call a tool, output a <tool_use> block:\n\n` +
    `<tool_use>\n{"name": "TOOL_NAME", "input": {ARGUMENTS}}\n</tool_use>\n\n` +
    `- You may write reasoning text before the block\n` +
    `- Call multiple tools by including multiple <tool_use> blocks\n` +
    `- Each block must be valid JSON with "name" and "input" keys\n\n` +
    defs
}

interface ToolCall { id: string; name: string; input: unknown }

function parseToolUse(text: string): { toolCalls: ToolCall[]; textBefore: string } {
  const regex = /<tool_use>([\s\S]*?)<\/tool_use>/g
  const calls: ToolCall[] = []
  let firstIdx = -1
  let m: RegExpExecArray | null
  while ((m = regex.exec(text)) !== null) {
    if (firstIdx < 0) firstIdx = m.index
    try {
      const p = JSON.parse(m[1]!.trim())
      calls.push({
        id: `toolu_${randomBytes(16).toString("hex")}`,
        name: String(p.name ?? ""),
        input: p.input ?? {}
      })
    } catch { /* skip malformed block */ }
  }
  return { toolCalls: calls, textBefore: firstIdx > 0 ? text.slice(0, firstIdx).trim() : "" }
}

function roughTokens(text: string): number {
  return Math.ceil((text ?? "").length / 4)
}

// ── Query options builder ────────────────────────────────────────────────────

function buildQueryOptions(model: "sonnet" | "opus" | "haiku", partial?: boolean, clientToolMode?: boolean) {
  if (clientToolMode) {
    // Single-turn, no MCP — caller manages the tool loop
    return {
      maxTurns: 1,
      model,
      pathToClaudeCodeExecutable: claudeExecutable,
      ...(partial ? { includePartialMessages: true } : {}),
      disallowedTools: [...BLOCKED_BUILTIN_TOOLS],
    }
  }
  return {
    maxTurns: 50,
    model,
    pathToClaudeCodeExecutable: claudeExecutable,
    ...(partial ? { includePartialMessages: true } : {}),
    disallowedTools: [...BLOCKED_BUILTIN_TOOLS],
    allowedTools: [...ALLOWED_MCP_TOOLS],
    mcpServers: {
      // Fresh instance per request — avoids concurrency issues when multiple
      // requests share the same SDK MCP server object.
      [MCP_SERVER_NAME]: createMcpServer()
    }
  }
}

// ── Route handler ────────────────────────────────────────────────────────────

export function createProxyServer(config: Partial<ProxyConfig> = {}) {
  const finalConfig = { ...DEFAULT_PROXY_CONFIG, ...config }
  const app = new Hono()

  app.use("*", cors())

  app.get("/", (c) => c.json({
    status: "ok",
    service: "claude-max-proxy",
    version: PROXY_VERSION,
    format: "anthropic",
    endpoints: ["/v1/messages", "/messages", "/v1/models", "/models"]
  }))

  const MODELS = [
    { type: "model", id: "claude-opus-4-6",              display_name: "Claude Opus 4.6",    created_at: "2025-08-01T00:00:00Z" },
    { type: "model", id: "claude-sonnet-4-6",            display_name: "Claude Sonnet 4.6",  created_at: "2025-08-01T00:00:00Z" },
    { type: "model", id: "claude-haiku-4-5",             display_name: "Claude Haiku 4.5",   created_at: "2025-10-01T00:00:00Z" },
    { type: "model", id: "claude-haiku-4-5-20251001",    display_name: "Claude Haiku 4.5",   created_at: "2025-10-01T00:00:00Z" },
  ]

  const handleModels = (c: Context) => c.json({ data: MODELS })
  app.get("/v1/models", handleModels)
  app.get("/models", handleModels)

  const handleModel = (c: Context) => {
    const id = c.req.param("id")
    const model = MODELS.find(m => m.id === id)
    if (!model) return c.json({ type: "error", error: { type: "not_found_error", message: `Model \`${id}\` not found` } }, 404)
    return c.json(model)
  }
  app.get("/v1/models/:id", handleModel)
  app.get("/models/:id", handleModel)

  // Rough token count — enough to satisfy clients that call this before streaming
  const handleCountTokens = async (c: Context) => {
    try {
      const body = await c.req.json()
      const sysText = Array.isArray(body.system)
        ? body.system.filter((b: any) => b.type === "text").map((b: any) => b.text).join("\n")
        : String(body.system ?? "")
      const msgText = (body.messages ?? [])
        .map((m: any) => typeof m.content === "string" ? m.content : JSON.stringify(m.content))
        .join("\n")
      const inputTokens = roughTokens(sysText + msgText)
      return c.json({ input_tokens: inputTokens })
    } catch {
      return c.json({ input_tokens: 0 })
    }
  }
  app.post("/v1/messages/count_tokens", handleCountTokens)
  app.post("/messages/count_tokens", handleCountTokens)

  const handleMessages = async (c: Context) => {
    const reqId = randomBytes(4).toString("hex")
    try {
      const body = await c.req.json()

      // Basic request validation
      if (!body.messages || !Array.isArray(body.messages) || body.messages.length === 0) {
        return c.json({ type: "error", error: { type: "invalid_request_error", message: "messages is required and must be a non-empty array" } }, 400)
      }

      const model = mapModelToClaudeModel(body.model || "sonnet")
      const stream = body.stream ?? true
      const clientToolMode = isClientToolMode(body)

      claudeLog("proxy.request", { reqId, model, stream, msgs: body.messages?.length, clientToolMode })

      const tempFiles: string[] = []

      // System prompt — extract text from string or block array
      let systemContext = ""
      if (body.system) {
        if (typeof body.system === "string") {
          systemContext = body.system
        } else if (Array.isArray(body.system)) {
          systemContext = body.system
            .filter((b: any) => b.type === "text" && b.text)
            .map((b: any) => b.text)
            .join("\n")
        }
      }

      // Conversation history
      const conversationParts = body.messages
        ?.map((m: { role: string; content: string | Array<any> }) => {
          const role = m.role === "assistant" ? "Assistant" : "Human"
          return `${role}: ${serializeContent(m.content, tempFiles)}`
        })
        .join("\n\n") || ""

      // Build prompt differently depending on mode:
      //   agent mode  → inject SEND_MESSAGE_NOTE for openclaw's MCP message tool
      //   client tool → inject caller's tool definitions; no MCP notes
      const toolsSection = clientToolMode ? buildClientToolsPrompt(body.tools) : SEND_MESSAGE_NOTE
      const prompt = systemContext
        ? `${systemContext}${toolsSection}\n\n${conversationParts}`
        : `${toolsSection}\n\n${conversationParts}`

      // ── Non-streaming ──────────────────────────────────────────────────────
      if (!stream) {
        // Reset on each assistant turn so we only return the FINAL turn's text.
        // This prevents intermediate tool-use narration ("I'll read the file…")
        // from leaking into the response when the agent does multi-turn work.
        let fullText = ""
        try {
          for await (const message of query({ prompt, options: buildQueryOptions(model, false, clientToolMode) })) {
            if (message.type === "assistant") {
              fullText = ""   // discard previous turn — keep only the last
              for (const block of message.message.content) {
                if (block.type === "text") fullText += block.text
              }
            }
          }
        } finally {
          cleanupTempFiles(tempFiles)
        }

        if (clientToolMode) {
          const { toolCalls, textBefore } = parseToolUse(fullText)
          const content: any[] = []
          if (textBefore) content.push({ type: "text", text: textBefore })
          for (const tc of toolCalls) content.push({ type: "tool_use", id: tc.id, name: tc.name, input: tc.input })
          if (content.length === 0) content.push({ type: "text", text: fullText || "..." })
          const stopReason = toolCalls.length > 0 ? "tool_use" : "end_turn"
          claudeLog("proxy.response", { reqId, len: fullText.length, toolCalls: toolCalls.length })
          return c.json({
            id: `msg_${Date.now()}`,
            type: "message", role: "assistant", content,
            model: body.model, stop_reason: stopReason, stop_sequence: null,
            usage: { input_tokens: roughTokens(prompt), output_tokens: roughTokens(fullText) }
          })
        }

        const normalized = normalizeResponse(fullText)
        claudeLog("proxy.response", { reqId, len: normalized.length })
        return c.json({
          id: `msg_${Date.now()}`,
          type: "message", role: "assistant",
          content: [{ type: "text", text: normalized }],
          model: body.model, stop_reason: "end_turn", stop_sequence: null,
          usage: { input_tokens: roughTokens(prompt), output_tokens: roughTokens(normalized) }
        })
      }

      // ── Streaming ──────────────────────────────────────────────────────────
      // message_start is emitted immediately so the client sees an active stream
      // before the agent does any work — prevents HTTP timeouts on long runs.
      //
      // Agent mode:      stream text deltas in real-time during the run.
      // Client tool mode: buffer output, then emit proper content blocks (text +
      //                   tool_use) at the end with correct stop_reason.
      const encoder = new TextEncoder()
      const readable = new ReadableStream({
        async start(controller) {
          const messageId = `msg_${Date.now()}`

          const sse = (event: string, data: object) => {
            try {
              controller.enqueue(encoder.encode(`event: ${event}\ndata: ${JSON.stringify(data)}\n\n`))
            } catch {}
          }

          try {
            const heartbeat = setInterval(() => {
              try { controller.enqueue(encoder.encode(": ping\n\n")) } catch { clearInterval(heartbeat) }
            }, 15_000)

            sse("message_start", {
              type: "message_start",
              message: {
                id: messageId, type: "message", role: "assistant", content: [],
                model: body.model, stop_reason: null, stop_sequence: null,
                usage: { input_tokens: roughTokens(prompt), output_tokens: 0 }
              }
            })

            // ── Client tool mode: buffer → emit blocks at end ───────────────
            if (clientToolMode) {
              let fullText = ""
              try {
                for await (const message of query({ prompt, options: buildQueryOptions(model, true, true) })) {
                  if (message.type === "stream_event") {
                    const ev = message.event as any
                    if (ev.type === "content_block_delta" && ev.delta?.type === "text_delta") {
                      fullText += ev.delta.text ?? ""
                    }
                  }
                }
              } finally {
                clearInterval(heartbeat)
                cleanupTempFiles(tempFiles)
              }

              const { toolCalls, textBefore } = parseToolUse(fullText)
              claudeLog("proxy.stream.done", { reqId, len: fullText.length, toolCalls: toolCalls.length })

              let blockIdx = 0
              // Emit text block if there's content before tool calls, or no tools
              const textContent = toolCalls.length === 0 ? normalizeResponse(fullText) : textBefore
              if (textContent) {
                sse("content_block_start", { type: "content_block_start", index: blockIdx, content_block: { type: "text", text: "" } })
                sse("content_block_delta", { type: "content_block_delta", index: blockIdx, delta: { type: "text_delta", text: textContent } })
                sse("content_block_stop", { type: "content_block_stop", index: blockIdx })
                blockIdx++
              } else if (toolCalls.length === 0) {
                // No text, no tools — emit fallback
                sse("content_block_start", { type: "content_block_start", index: 0, content_block: { type: "text", text: "" } })
                sse("content_block_delta", { type: "content_block_delta", index: 0, delta: { type: "text_delta", text: "..." } })
                sse("content_block_stop", { type: "content_block_stop", index: 0 })
                blockIdx = 1
              }
              // Emit tool_use blocks
              for (const tc of toolCalls) {
                sse("content_block_start", { type: "content_block_start", index: blockIdx, content_block: { type: "tool_use", id: tc.id, name: tc.name, input: "" } })
                sse("content_block_delta", { type: "content_block_delta", index: blockIdx, delta: { type: "input_json_delta", partial_json: JSON.stringify(tc.input) } })
                sse("content_block_stop", { type: "content_block_stop", index: blockIdx })
                blockIdx++
              }

              const stopReason = toolCalls.length > 0 ? "tool_use" : "end_turn"
              sse("message_delta", { type: "message_delta", delta: { stop_reason: stopReason, stop_sequence: null }, usage: { output_tokens: roughTokens(fullText) } })
              sse("message_stop", { type: "message_stop" })
              controller.close()
              return
            }

            // ── Agent mode: stream text deltas in real-time ─────────────────
            sse("content_block_start", { type: "content_block_start", index: 0, content_block: { type: "text", text: "" } })

            let fullText = ""
            try {
              for await (const message of query({ prompt, options: buildQueryOptions(model, true) })) {
                if (message.type === "stream_event") {
                  const ev = message.event as any
                  if (ev.type === "content_block_delta" && ev.delta?.type === "text_delta") {
                    const chunk = ev.delta.text ?? ""
                    if (chunk) {
                      fullText += chunk
                      sse("content_block_delta", { type: "content_block_delta", index: 0, delta: { type: "text_delta", text: chunk } })
                    }
                  }
                }
              }
            } finally {
              clearInterval(heartbeat)
              cleanupTempFiles(tempFiles)
            }

            // If agent produced nothing (maxTurns exhausted, etc.) emit "..." fallback.
            // NO_REPLY: already streamed live — openclaw detects "ends with NO_REPLY" on its end.
            const normalized = normalizeResponse(fullText)
            claudeLog("proxy.stream.done", { reqId, len: fullText.length, normalized: normalized !== fullText })

            if (!fullText || !fullText.trim()) {
              sse("content_block_delta", { type: "content_block_delta", index: 0, delta: { type: "text_delta", text: "..." } })
            }

            sse("content_block_stop", { type: "content_block_stop", index: 0 })
            sse("message_delta", { type: "message_delta", delta: { stop_reason: "end_turn", stop_sequence: null }, usage: { output_tokens: roughTokens(fullText) } })
            sse("message_stop", { type: "message_stop" })
            controller.close()

          } catch (error) {
            claudeLog("proxy.stream.error", { reqId, error: error instanceof Error ? error.message : String(error) })
            cleanupTempFiles(tempFiles)
            try {
              sse("error", { type: "error", error: { type: "api_error", message: error instanceof Error ? error.message : "Unknown error" } })
              controller.close()
            } catch {}
          }
        }
      })

      return new Response(readable, {
        headers: {
          "Content-Type": "text/event-stream",
          "Cache-Control": "no-cache",
          "Connection": "keep-alive"
        }
      })

    } catch (error) {
      claudeLog("proxy.error", { reqId, error: error instanceof Error ? error.message : String(error) })
      return c.json({
        type: "error",
        error: { type: "api_error", message: error instanceof Error ? error.message : "Unknown error" }
      }, 500)
    }
  }

  app.post("/v1/messages", handleMessages)
  app.post("/messages", handleMessages)

  return { app, config: finalConfig }
}

export async function startProxyServer(config: Partial<ProxyConfig> = {}) {
  const { app, config: finalConfig } = createProxyServer(config)

  const server = Bun.serve({
    port: finalConfig.port,
    hostname: finalConfig.host,
    fetch: app.fetch,
    idleTimeout: 0
  })

  console.log(`Claude Max Proxy running at http://${finalConfig.host}:${finalConfig.port}`)
  return server
}
